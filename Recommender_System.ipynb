{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a movie recommendation system based on tweets\n",
    "> You may want to take a look to the previous notebok \"Sentiment_Analysis.ipynb\" before we start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: textblob in d:\\programdata\\anaconda3\\lib\\site-packages (0.15.3)\n",
      "Requirement already satisfied, skipping upgrade: nltk>=3.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from textblob) (3.4.5)\n",
      "Requirement already satisfied, skipping upgrade: six in d:\\programdata\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.12.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tweepy in d:\\programdata\\anaconda3\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from tweepy) (2.22.0)\n",
      "Requirement already satisfied: six>=1.10.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from tweepy) (1.12.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from tweepy) (1.3.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (2019.9.11)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in d:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Robert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Robert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Robert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Robert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\Robert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Robert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in d:\\programdata\\anaconda3\\lib\\site-packages (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in d:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.16.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already up-to-date: scikit-learn in d:\\programdata\\anaconda3\\lib\\site-packages (0.24.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in d:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in d:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in d:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in d:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.16.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U textblob\n",
    "%pip install tweepy\n",
    "!python -m textblob.download_corpora\n",
    "%pip install -U scikit-learn\n",
    "%pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Robert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Robert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Robert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Robert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import nltk\n",
    "import tweepy\n",
    "\n",
    "nltk.download(['wordnet', 'punkt', 'averaged_perceptron_tagger', 'stopwords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Importing the models\n",
    "> We can import the model \"svcmodel.joblib\" from the previous section or use the TextBlob default model (NaiveBayesAnalyzer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "classifier = joblib.load('trained_models/svcmodel.joblib')\n",
    "tfidf = joblib.load('trained_models/tfidf.joblib') #it is necessary for the test cases to share the same TF-IDF vectorizer with which was trained the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TextBlob default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob, Blobber\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "classifier2 = Blobber(analyzer=NaiveBayesAnalyzer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Movies dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                original_title                    genres\n",
      "1                      Jumanji  Adventure Fantasy Family\n",
      "2             Grumpier Old Men            Romance Comedy\n",
      "3            Waiting to Exhale      Comedy Drama Romance\n",
      "4  Father of the Bride Part II                    Comedy\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ast\n",
    "\n",
    "# this function is used to convert the genres column (in JSON format) to a string separated by whitespaces\n",
    "def parse_json(text):\n",
    "        text = ast.literal_eval(text)\n",
    "\n",
    "        r = []\n",
    "        for i in text:\n",
    "            i = str(i).replace(\"\\'\", \"\\\"\")\n",
    "            movie = json.loads(i)\n",
    "            r.append(movie['name'])\n",
    "\n",
    "        return \" \".join(r)\n",
    "\n",
    "dataset = pd.read_csv('datasets/movies_metadata.csv', dtype=str).loc[:8000, ['original_title', 'genres']].dropna()\n",
    "dataset['genres'] = dataset['genres'].apply(lambda x: parse_json(x))\n",
    "\n",
    "print(dataset[1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up tweepy API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler\n",
    "\n",
    "consumer_key = 'AAAA'\n",
    "consumer_secret = 'BBBB'\n",
    "access_token = 'CCCC'\n",
    "access_token_secret = 'DDDD'\n",
    "\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ace Ventura. Neither terrible, boring nor soporific, just not very funny. \n",
      "\n",
      "Jumanji, with plenty of laughs, action-packed excitement, great music, spectacular sets, and inspirational themes, this film is an absolutely winning adventure. \n",
      "\n",
      "Die Hard. There are good performances from everyone in this long, often funny, very violent but exciting melodrama. \n",
      "\n",
      "Meet Joe Black. I've never encountered such dramatic flatulence, never heard so many pregnant silences that don't deliver, never watched so many close-ups that graze on actors' faces until every last trace of expression has been devoured. \n",
      "\n",
      "Toy Story is a Pixar classic, one of the best kids' movies of all time. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "username = 'DevFarzan' #Replace by the desired username\n",
    "count = 5 #Replace by the desired number of tweets to analyze\n",
    "\n",
    "tweets = api.user_timeline(screen_name=username, count=count, include_rts = False, tweet_mode = 'extended')\n",
    "tweets = list(map(lambda tw: tw.full_text, tweets))\n",
    "\n",
    "for tweet in tweets:\n",
    "    print(tweet, '\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze tweets' sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning links, @ users, html tags and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean links, @ users, html tags and special characters\n",
    "def clean_text(raw_text):\n",
    "    clean = re.compile(\"<.*?>|([^A-Za-z'])|('s)\")\n",
    "    cleantext = re.sub(clean, ' ', raw_text)\n",
    "    cleantext = \" \".join(re.split('[!?\\., ]', cleantext))\n",
    "    cleantext = re.sub(r'\\s+', ' ', cleantext)\n",
    "    cleantext = re.sub(\"\\s\\W+\\s\", ' ', cleantext)\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stopwords from text (except useful words like not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#remove stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    \n",
    "    all_stopwords = stopwords.words('english')\n",
    "    all_stopwords.remove('not') #excluding not as stopword\n",
    "\n",
    "    negation = False\n",
    "    result = []\n",
    "    delims = \"?.,!:; \"\n",
    "    \n",
    "    # no permitimos que las negaciones sean borradas de los textos\n",
    "    for word in text.split():\n",
    "        stripped = word.strip(delims).lower()\n",
    "        negated = \"not \" + stripped if negation else stripped\n",
    "        negated = re.sub(\"n\\'t\", \" not\", stripped)\n",
    "        negated = re.sub(\"'ve\", \" have\", stripped)\n",
    "        result.append(negated)\n",
    "        \n",
    "        if any(neg in word for neg in [\"not\", \"n't\", \"no\"]):\n",
    "            negation = not negation\n",
    "\n",
    "        if any(c in word for c in delims):\n",
    "            negation = False\n",
    "\n",
    "    text = [word for word in result if not word in set(all_stopwords)]\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract tag from WordNet\n",
    "def wordnet_tag(text):\n",
    "    \n",
    "    #extrae el tag de wordnet del string devuelto por nltk.pos_tag\n",
    "    if text.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif text.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif text.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif text.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "#Text lemmatization\n",
    "def lemmatize_text(text):\n",
    "\n",
    "    lem = WordNetLemmatizer()\n",
    "    tag_text = nltk.pos_tag(nltk.word_tokenize(text))\n",
    "    text = map(lambda x: (x[0], wordnet_tag(x[1])), tag_text)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in text:\n",
    "        if tag is None:\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            lemmatized_sentence.append(lem.lemmatize(word, tag))\n",
    "\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ace ventura neither terrible boring soporific not funny', 'jumanji plenty laugh action pack excitement great music spectacular set inspirational theme film absolutely win adventure', 'die hard good performance everyone long often funny violent excite melodrama', \"meet joe black i have never encounter dramatic flatulence never hear many pregnant silence deliver never watch many close ups graze actor ' face every last trace expression devour\", \"toy story pixar classic one best kid ' movie time\"]\n"
     ]
    }
   ],
   "source": [
    "#assembling previous functions into one\n",
    "def preprocess_text(text):\n",
    "    text = remove_stopwords(text)\n",
    "    text = clean_text(text)\n",
    "    text = lemmatize_text(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "test = list(map(lambda text: preprocess_text(text), tweets))\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC classifier:  ['neg', 'pos', 'pos', 'pos', 'pos']\n",
      "TextBlob NB Classifier:  ['neg', 'pos', 'pos', 'pos', 'pos']\n"
     ]
    }
   ],
   "source": [
    "# vectorize test array into TF-IDF\n",
    "tftest = tfidf.transform(test).toarray()\n",
    "\n",
    "#analyze sentiment\n",
    "sentiment = ['pos' if x == 1 else 'neg' for x in classifier.predict(tftest)]\n",
    "\n",
    "print(\"SVC classifier: \", sentiment)\n",
    "\n",
    "#try the textblob sentiment classifier\n",
    "s = []\n",
    "for x in tweets:\n",
    "    s.append(classifier2(x).sentiment.classification)\n",
    "\n",
    "print(\"TextBlob NB Classifier: \", s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Ace Ventura. Neither terrible, boring nor soporific, just not very funny.'\n",
      "  'neg']\n",
      " ['Jumanji, with plenty of laughs, action-packed excitement, great music, spectacular sets, and inspirational themes, this film is an absolutely winning adventure.'\n",
      "  'pos']\n",
      " ['Die Hard. There are good performances from everyone in this long, often funny, very violent but exciting melodrama.'\n",
      "  'pos']\n",
      " [\"Meet Joe Black. I've never encountered such dramatic flatulence, never heard so many pregnant silences that don't deliver, never watched so many close-ups that graze on actors' faces until every last trace of expression has been devoured.\"\n",
      "  'pos']\n",
      " [\"Toy Story is a Pixar classic, one of the best kids' movies of all time.\"\n",
      "  'pos']]\n"
     ]
    }
   ],
   "source": [
    "#append sentiment to review\n",
    "\n",
    "tw = np.array(tweets)\n",
    "sent = np.array(sentiment)\n",
    "\n",
    "reviews = np.hstack((tw.reshape((len(tw), 1)), sent.reshape((len(sent), 1))))\n",
    "\n",
    "print(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get recommendations based on positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Jumanji, with plenty of laughs, action-packed excitement, great music, spectacular sets, and inspirational themes, this film is an absolutely winning adventure.'\n",
      "  'pos']\n",
      " ['Die Hard. There are good performances from everyone in this long, often funny, very violent but exciting melodrama.'\n",
      "  'pos']\n",
      " [\"Meet Joe Black. I've never encountered such dramatic flatulence, never heard so many pregnant silences that don't deliver, never watched so many close-ups that graze on actors' faces until every last trace of expression has been devoured.\"\n",
      "  'pos']\n",
      " [\"Toy Story is a Pixar classic, one of the best kids' movies of all time.\"\n",
      "  'pos']]\n"
     ]
    }
   ],
   "source": [
    "# get the positive tweets\n",
    "positive_reviews = np.array(list(filter(lambda x: x[1] == 'pos', reviews)))\n",
    "\n",
    "print(positive_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the movie (in the dataset) mentioned in the tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Warning:** This function may fail to find the right movie sometimes because it does a linear search on the dataset, which means that if another title matches in the review, it will choose the first coincidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, original_title                     jumanji\n",
      "genres            Adventure Fantasy Family\n",
      "Name: 1, dtype: object), (1007, original_title           die hard\n",
      "genres            Action Thriller\n",
      "Name: 1007, dtype: object), (688, original_title    faces\n",
      "genres            Drama\n",
      "Name: 688, dtype: object), (0, original_title                  toy story\n",
      "genres            Animation Comedy Family\n",
      "Name: 0, dtype: object)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#search movie titles in the dataset using regex, and return the movie index of the dataset\n",
    "def is_movie_tweet(text):\n",
    "\n",
    "    dataset['original_title'] = dataset.loc[:,'original_title'].str.lower()\n",
    "    for series in dataset.iterrows():\n",
    "        t = r\"\\b\" + re.escape(series[1]['original_title']) + r\"\\b\"\n",
    "        if re.search(t, text.lower()) != None:\n",
    "            return series\n",
    "        \n",
    "    return False\n",
    "\n",
    "movies = list(filter(lambda x: x != False, [is_movie_tweet(text) for text in positive_reviews[:, 0]]))\n",
    "\n",
    "print(movies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content based filtering recommendations\n",
    "We base our recommendations on the genres of the liked movies to perform cosine similarity on them and find movies with very similar genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "    \n",
    "def similarContentBased(movieId):\n",
    "\n",
    "    cv = CountVectorizer()\n",
    "    X = cv.fit_transform(dataset['genres']).toarray()[:10000]\n",
    "\n",
    "    sim = cosine_similarity(X)\n",
    "    sim = pd.Series(sim[movieId]).sort_values(ascending=False)\n",
    "    indexes = list(sim.index)\n",
    "\n",
    "    limit = 5\n",
    "    recommendations = []\n",
    "\n",
    "    i = 0\n",
    "    while(i < limit):\n",
    "        ind = indexes[i]\n",
    "        if ind != movieId:\n",
    "            movie = dataset.loc[ind, ['original_title', 'genres']]\n",
    "            recommendations.append((movie[0], movie[1], sim[ind]))\n",
    "        else:\n",
    "            limit += 1\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('return to oz', 'Adventure Family Fantasy', 1.0000000000000002), ('peter pan', 'Adventure Fantasy Family', 1.0000000000000002), ('harry potter and the prisoner of azkaban', 'Adventure Fantasy Family', 1.0000000000000002), ('jason and the argonauts', 'Adventure Family Fantasy', 1.0000000000000002), ('clash of the titans', 'Adventure Fantasy Family', 1.0000000000000002)], [('on deadly ground', 'Action Thriller', 0.9999999999999998), ('air force one', 'Action Thriller', 0.9999999999999998), ('iron eagle iii', 'Action Thriller', 0.9999999999999998), ('the peacemaker', 'Action Thriller', 0.9999999999999998), ('d-tox', 'Action Thriller', 0.9999999999999998)], [('the hours', 'Drama', 1.0), ('querelle', 'Drama', 1.0), ('dead poets society', 'Drama', 1.0), ('the graduate', 'Drama', 1.0), ('coming apart', 'Drama', 1.0)], [('the great mouse detective', 'Comedy Animation Family', 1.0000000000000002), ('the wrong trousers', 'Animation Comedy Family', 1.0000000000000002), (\"bon voyage, charlie brown (and don't come back!)\", 'Animation Comedy Family', 1.0000000000000002), ('creature comforts', 'Animation Comedy Family', 1.0000000000000002), ('a close shave', 'Family Animation Comedy', 1.0000000000000002)]]\n"
     ]
    }
   ],
   "source": [
    "recommendations = [similarContentBased(r[0]) for r in movies]\n",
    "\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Movie: \u001b[1mJumanji\u001b[0m | Genres: Adventure Fantasy Family | Index: 1\n",
      "\n",
      "\t*Recommendation: \u001b[1mReturn To Oz\u001b[0m | Genres: Adventure Family Fantasy | Similarity: 1.0\n",
      "\n",
      "\t*Recommendation: \u001b[1mPeter Pan\u001b[0m | Genres: Adventure Fantasy Family | Similarity: 1.0\n",
      "\n",
      "\t*Recommendation: \u001b[1mHarry Potter And The Prisoner Of Azkaban\u001b[0m | Genres: Adventure Fantasy Family | Similarity: 1.0\n",
      "\n",
      "\t*Recommendation: \u001b[1mJason And The Argonauts\u001b[0m | Genres: Adventure Family Fantasy | Similarity: 1.0\n",
      "\n",
      "\t*Recommendation: \u001b[1mClash Of The Titans\u001b[0m | Genres: Adventure Fantasy Family | Similarity: 1.0\n",
      "\n",
      "-- Movie: \u001b[1mDie Hard\u001b[0m | Genres: Action Thriller | Index: 1007\n",
      "\n",
      "\t*Recommendation: \u001b[1mOn Deadly Ground\u001b[0m | Genres: Action Thriller | Similarity: 1.0\n",
      "\n",
      "\t*Recommendation: \u001b[1mAir Force One\u001b[0m | Genres: Action Thriller | Similarity: 1.0\n",
      "\n",
      "\t*Recommendation: \u001b[1mIron Eagle Iii\u001b[0m | Genres: Action Thriller | Similarity: 1.0\n",
      "\n",
      "\t*Recommendation: \u001b[1mThe Peacemaker\u001b[0m | Genres: Action Thriller | Similarity: 1.0\n",
      "\n",
      "\t*Recommendation: \u001b[1mD-Tox\u001b[0m | Genres: Action Thriller | Similarity: 1.0\n",
      "\n",
      "-- Movie: \u001b[1mFaces\u001b[0m | Genres: Drama | Index: 688\n",
      "\n",
      "\t*Recommendation: \u001b[1mThe Hours\u001b[0m | Genres: Drama | Similarity: 1.0\n",
      "\n",
      "\t*Recommendation: \u001b[1mQuerelle\u001b[0m | Genres: Drama | Similarity: 1.0\n",
      "\n",
      "\t*Recommendation: \u001b[1mDead Poets Society\u001b[0m | Genres: Drama | Similarity: 1.0\n",
      "\n",
      "\t*Recommendation: \u001b[1mThe Graduate\u001b[0m | Genres: Drama | Similarity: 1.0\n",
      "\n",
      "\t*Recommendation: \u001b[1mComing Apart\u001b[0m | Genres: Drama | Similarity: 1.0\n",
      "\n",
      "-- Movie: \u001b[1mToy Story\u001b[0m | Genres: Animation Comedy Family | Index: 0\n",
      "\n",
      "\t*Recommendation: \u001b[1mThe Great Mouse Detective\u001b[0m | Genres: Comedy Animation Family | Similarity: 1.0\n",
      "\n",
      "\t*Recommendation: \u001b[1mThe Wrong Trousers\u001b[0m | Genres: Animation Comedy Family | Similarity: 1.0\n",
      "\n",
      "\t*Recommendation: \u001b[1mBon Voyage, Charlie Brown (And Don'T Come Back!)\u001b[0m | Genres: Animation Comedy Family | Similarity: 1.0\n",
      "\n",
      "\t*Recommendation: \u001b[1mCreature Comforts\u001b[0m | Genres: Animation Comedy Family | Similarity: 1.0\n",
      "\n",
      "\t*Recommendation: \u001b[1mA Close Shave\u001b[0m | Genres: Family Animation Comedy | Similarity: 1.0\n"
     ]
    }
   ],
   "source": [
    "#printing each movie with its recommendations\n",
    "\n",
    "for i, movie in enumerate(movies):\n",
    "    print(\"\\n-- Movie: \\033[1m\" + movie[1]['original_title'].title() + \\\n",
    "          \"\\033[0m | Genres: \" + movie[1]['genres'] + \" | Index: \" + str(movie[0]))\n",
    "    \n",
    "    for rec in recommendations[i]:\n",
    "        print(\"\\n\\t*Recommendation: \\033[1m\" + rec[0].title() + \\\n",
    "              \"\\033[0m | Genres: \" + rec[1] + \" | Similarity: \" + str(round(rec[2], 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
